# Computer Vision - Portfolio Exam 3

# Variational Autoencoders (VAE) with β-VAE Variants - CelebA Dataset Analysis

**Submitted by:**

* Harshith Babu Prakash Babu - 10001191
* Riya Biju - 10000742
* Harsha Sathish - 10001000


---

## Project Overview

This project implements and analyzes Variational Autoencoders (VAE) and β-VAE variants for image generation and representation learning on the CelebA dataset. The study explores the impact of latent dimensionality and disentanglement parameter (β) on reconstruction quality, generation capability, and latent space structure.

---

## Prerequisites

**Dataset:**

Please download the CelebA - (Aligned + Cropped version) from the link given -> https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html:
**Python Environment:**

Create a virtual environment and install dependencies:

```bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
# On Windows:
venv\Scripts\activate
# On Linux/Mac:
source venv/bin/activate

# Install required packages
pip install -r requirements.txt
```

---

## Repository Structure

```
Portfolio3_Harsha_Riya_Harshith/
│
├── README.md                                   # This file
├── requirements.txt                            # Python dependencies
│
├── presentation.pdf                           # Presentation file
├── part2_theory.pdf                           # Part 2 Theory Explanation
│
├── task1_main.ipynb                           # Baseline VAE (128-dim, β=1, 50 epochs)
├── task1_main_100_epochs.ipynb                # Extended baseline (100 epochs)
│
├── task1_64dim.ipynb                          # Latent dim = 64
├── task1_256.ipynb                            # Latent dim = 256
├── task1_512.ipynb                            # Latent dim = 512
│
├── task1_beta_0.5.ipynb                       # β-VAE (β = 0.5)
├── task1_beta_4.ipynb                         # β-VAE (β = 4)
├── task1_beta_10.ipynb                        # β-VAE (β = 10)
│
├── comparison_latent_dim/                     # Comparative analysis: latent dimensions
├── comparison_beta_value/                     # Comparative analysis: β values
│
├── outputs_baseline_50epochs/                 # Results: baseline 50 epochs
│   ├── best_model.pth                         # Cloned Repo Structure for all output dir
│   ├── training_curves.png
│   ├── training_history.json
│   ├── reconstruction_test_set.png
│   ├── random_sample_latentspace.png
│   ├── latent_space_traversals.png
│   ├── latent_dim_traversals.png
│   └── samples/                               # Epoch-wise reconstructions (every 5 epochs)
│
├── outputs_baseline_100epochs/                # Results: baseline 100 epochs
├── outputs_64/                                # Results: 64-dim latent space
├── outputs_256/                               # Results: 256-dim latent space
├── outputs_512/                               # Results: 512-dim latent space
├── outputs_beta_0.5/                          # Results: β = 0.5
├── outputs_beta_4/                            # Results: β = 4
└── outputs_beta_10/                           # Results: β = 10
```

**Note:** Each `outputs_*` folder follows the same structure as `outputs_baseline_50epochs/`

---

## Output Files

Each experimental configuration generates the following outputs:

1. **best_model.pth**
   * Saved model checkpoint with best validation performance
2. **training_history.json**
   * Detailed training metrics (loss components, epochs, hyperparameters)
3. **training_curves.png**
   * Training and validation loss curves over epochs
4. **reconstruction_test_set.png**
   * Side-by-side comparison of original vs reconstructed test images
5. **random_sample_latentspace.png**
   * Faces generated by sampling from learned latent distribution N(0,I)
6. **latent_space_traversals.png**
   * Traversal along individual latent dimensions
7. **latent_dim_traversals.png**
   * Interpolation between encoded test images in latent space
8. **samples/** (folder)
   * Progressive reconstruction quality snapshots
   * Saved every 5 epochs during training
---

## Acknowledgments

* **Instructor:** Prof. Dr. Dominik Seuß - Computer Vision Course, THWS
* **Dataset:** CelebA (Large-scale CelebFaces Attributes Dataset)
* **Reference Paper:** β-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework (Higgins et al., ICLR 2017)

---

## Submitted by:

* Riya Biju
* Harsha Sathish
* Harshith Babu Prakash Babu

**Course:** Computer Vision - Portfolio Exam 3  
**Institution:** Technische Hochschule Würzburg-Schweinfurt (THWS)  
**Program:** Master of Science - Artificial Intelligence
