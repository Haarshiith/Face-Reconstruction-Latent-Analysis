{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd57a158",
   "metadata": {},
   "source": [
    "# <center> </center>\n",
    "# <center> **Computer Vision** </center>\n",
    "# <center> **Portfolio Exam 3**</center>\n",
    "# <center>**Implement and analyze a Variational Autoencoder on CelebA**</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2fc9c3",
   "metadata": {},
   "source": [
    "**Submitted by:**\n",
    "****\n",
    "\n",
    "*   **Riya Biju - 10000742**\n",
    "*   **Harsha Sathish - 10001000**\n",
    "*   **Harshith Babu Prakash Babu - 10001191**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bff6b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "============================================================\n",
      "VAE Training Configuration\n",
      "============================================================\n",
      "latent_dim          : 128\n",
      "beta                : 1.0\n",
      "img_size            : 64\n",
      "batch_size          : 128\n",
      "num_epochs          : 50\n",
      "learning_rate       : 0.001\n",
      "subset_train        : 50000\n",
      "subset_val          : 5000\n",
      "num_workers         : 0\n",
      "save_interval       : 5\n",
      "============================================================\n",
      "\n",
      "Loading datasets...\n",
      "TRAIN set: 50000 images\n",
      "VAL set: 5000 images\n",
      "\n",
      "Initializing model...\n",
      "Total parameters: 5,777,731\n",
      "Trainable parameters: 5,777,731\n",
      "\n",
      "============================================================\n",
      "Starting Training\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/391 [00:00<?, ?it/s]/Users/harshasathish/Desktop/MAI/ComputerVision/Portfolio3/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Epoch 1: 100%|██████████| 391/391 [02:02<00:00,  3.19it/s, Loss=6792.6978, Recon=0.5463, KL=79.9877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50:\n",
      "  Train - Loss: 6878.3105, Recon: 0.5542, KL: 68.1596\n",
      "  Val   - Loss: 6639.6001, Recon: 0.5341, KL: 76.7717\n",
      "  ⭐ New best model saved! Val Loss: 6639.6001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 391/391 [02:01<00:00,  3.21it/s, Loss=6434.8579, Recon=0.5173, KL=78.1094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50:\n",
      "  Train - Loss: 6557.9734, Recon: 0.5273, KL: 77.9249\n",
      "  Val   - Loss: 6523.7637, Recon: 0.5243, KL: 80.6387\n",
      "  ⭐ New best model saved! Val Loss: 6523.7637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 391/391 [02:02<00:00,  3.20it/s, Loss=6311.2368, Recon=0.5072, KL=78.4920]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50:\n",
      "  Train - Loss: 6478.5579, Recon: 0.5208, KL: 78.3853\n",
      "  Val   - Loss: 6481.7223, Recon: 0.5213, KL: 76.4383\n",
      "  ⭐ New best model saved! Val Loss: 6481.7223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 391/391 [02:02<00:00,  3.20it/s, Loss=6243.8496, Recon=0.5020, KL=75.1776]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50:\n",
      "  Train - Loss: 6443.6789, Recon: 0.5181, KL: 76.8124\n",
      "  Val   - Loss: 6452.3402, Recon: 0.5189, KL: 75.6709\n",
      "  ⭐ New best model saved! Val Loss: 6452.3402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 391/391 [02:03<00:00,  3.17it/s, Loss=6400.8022, Recon=0.5147, KL=76.4400]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50:\n",
      "  Train - Loss: 6421.0561, Recon: 0.5163, KL: 76.5419\n",
      "  Val   - Loss: 6422.8854, Recon: 0.5165, KL: 75.8616\n",
      "  Saved checkpoint: ./outputs/vae_baseline_latent128_20251231_170110/checkpoints/checkpoint_epoch_5.pth\n",
      "  ⭐ New best model saved! Val Loss: 6422.8854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 391/391 [02:12<00:00,  2.94it/s, Loss=6340.9531, Recon=0.5096, KL=79.1084]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50:\n",
      "  Train - Loss: 6406.0550, Recon: 0.5151, KL: 76.3173\n",
      "  Val   - Loss: 6416.3697, Recon: 0.5158, KL: 78.7179\n",
      "  ⭐ New best model saved! Val Loss: 6416.3697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 391/391 [02:02<00:00,  3.20it/s, Loss=6448.1924, Recon=0.5185, KL=76.6198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50:\n",
      "  Train - Loss: 6393.4945, Recon: 0.5141, KL: 76.8150\n",
      "  Val   - Loss: 6403.3099, Recon: 0.5148, KL: 77.5588\n",
      "  ⭐ New best model saved! Val Loss: 6403.3099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 391/391 [02:04<00:00,  3.15it/s, Loss=6357.6821, Recon=0.5110, KL=79.0492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50:\n",
      "  Train - Loss: 6387.0419, Recon: 0.5135, KL: 77.5937\n",
      "  Val   - Loss: 6397.5758, Recon: 0.5142, KL: 78.6530\n",
      "  ⭐ New best model saved! Val Loss: 6397.5758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 391/391 [02:06<00:00,  3.08it/s, Loss=6331.4570, Recon=0.5089, KL=77.5847]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50:\n",
      "  Train - Loss: 6380.1822, Recon: 0.5129, KL: 78.1107\n",
      "  Val   - Loss: 6398.4593, Recon: 0.5143, KL: 78.4119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 391/391 [02:09<00:00,  3.01it/s, Loss=6318.4966, Recon=0.5076, KL=81.3236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50:\n",
      "  Train - Loss: 6373.8249, Recon: 0.5123, KL: 78.9392\n",
      "  Val   - Loss: 6400.8673, Recon: 0.5143, KL: 80.8845\n",
      "  Saved checkpoint: ./outputs/vae_baseline_latent128_20251231_170110/checkpoints/checkpoint_epoch_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 391/391 [02:12<00:00,  2.94it/s, Loss=6481.8784, Recon=0.5211, KL=78.0648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/50:\n",
      "  Train - Loss: 6370.1122, Recon: 0.5119, KL: 79.5038\n",
      "  Val   - Loss: 6389.9251, Recon: 0.5135, KL: 79.7429\n",
      "  ⭐ New best model saved! Val Loss: 6389.9251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 391/391 [02:06<00:00,  3.10it/s, Loss=6162.7178, Recon=0.4950, KL=80.1881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/50:\n",
      "  Train - Loss: 6365.3154, Recon: 0.5115, KL: 79.7906\n",
      "  Val   - Loss: 6393.3338, Recon: 0.5136, KL: 81.6352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 391/391 [02:03<00:00,  3.17it/s, Loss=6268.4878, Recon=0.5035, KL=81.7440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/50:\n",
      "  Train - Loss: 6362.1873, Recon: 0.5112, KL: 79.9884\n",
      "  Val   - Loss: 6379.9578, Recon: 0.5127, KL: 80.1790\n",
      "  ⭐ New best model saved! Val Loss: 6379.9578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 391/391 [02:03<00:00,  3.17it/s, Loss=6289.6118, Recon=0.5053, KL=80.1810]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/50:\n",
      "  Train - Loss: 6360.0893, Recon: 0.5111, KL: 80.1438\n",
      "  Val   - Loss: 6383.0636, Recon: 0.5127, KL: 83.4185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 391/391 [02:08<00:00,  3.04it/s, Loss=6440.8608, Recon=0.5177, KL=79.9152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/50:\n",
      "  Train - Loss: 6355.8921, Recon: 0.5107, KL: 80.4546\n",
      "  Val   - Loss: 6374.5516, Recon: 0.5124, KL: 78.3633\n",
      "  Saved checkpoint: ./outputs/vae_baseline_latent128_20251231_170110/checkpoints/checkpoint_epoch_15.pth\n",
      "  ⭐ New best model saved! Val Loss: 6374.5516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 391/391 [02:09<00:00,  3.02it/s, Loss=6210.6250, Recon=0.4989, KL=79.6375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/50:\n",
      "  Train - Loss: 6354.3516, Recon: 0.5105, KL: 80.7783\n",
      "  Val   - Loss: 6374.2113, Recon: 0.5121, KL: 81.2469\n",
      "  ⭐ New best model saved! Val Loss: 6374.2113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 391/391 [02:05<00:00,  3.11it/s, Loss=6506.7666, Recon=0.5230, KL=80.1570]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/50:\n",
      "  Train - Loss: 6351.1403, Recon: 0.5103, KL: 81.0663\n",
      "  Val   - Loss: 6378.2788, Recon: 0.5127, KL: 78.5737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 391/391 [02:04<00:00,  3.14it/s, Loss=6325.9888, Recon=0.5080, KL=83.4667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/50:\n",
      "  Train - Loss: 6349.8823, Recon: 0.5101, KL: 81.2761\n",
      "  Val   - Loss: 6374.5412, Recon: 0.5120, KL: 83.4033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 391/391 [02:02<00:00,  3.19it/s, Loss=6458.6348, Recon=0.5190, KL=81.2497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/50:\n",
      "  Train - Loss: 6347.5917, Recon: 0.5099, KL: 81.4595\n",
      "  Val   - Loss: 6371.7635, Recon: 0.5118, KL: 82.2805\n",
      "  ⭐ New best model saved! Val Loss: 6371.7635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 391/391 [02:02<00:00,  3.19it/s, Loss=6320.8408, Recon=0.5076, KL=83.4030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/50:\n",
      "  Train - Loss: 6345.8173, Recon: 0.5098, KL: 81.6166\n",
      "  Val   - Loss: 6369.5498, Recon: 0.5118, KL: 80.6988\n",
      "  Saved checkpoint: ./outputs/vae_baseline_latent128_20251231_170110/checkpoints/checkpoint_epoch_20.pth\n",
      "  ⭐ New best model saved! Val Loss: 6369.5498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 391/391 [02:05<00:00,  3.12it/s, Loss=6577.0825, Recon=0.5287, KL=80.2349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/50:\n",
      "  Train - Loss: 6345.1591, Recon: 0.5097, KL: 81.7293\n",
      "  Val   - Loss: 6369.9380, Recon: 0.5117, KL: 81.6618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 391/391 [02:04<00:00,  3.14it/s, Loss=6315.1074, Recon=0.5073, KL=81.3754]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/50:\n",
      "  Train - Loss: 6342.4308, Recon: 0.5095, KL: 81.7900\n",
      "  Val   - Loss: 6368.3759, Recon: 0.5116, KL: 81.9110\n",
      "  ⭐ New best model saved! Val Loss: 6368.3759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 391/391 [02:05<00:00,  3.12it/s, Loss=6284.9302, Recon=0.5048, KL=81.7567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/50:\n",
      "  Train - Loss: 6341.1791, Recon: 0.5094, KL: 81.9361\n",
      "  Val   - Loss: 6364.1654, Recon: 0.5113, KL: 81.6104\n",
      "  ⭐ New best model saved! Val Loss: 6364.1654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 391/391 [02:02<00:00,  3.19it/s, Loss=6351.8550, Recon=0.5103, KL=80.9809]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/50:\n",
      "  Train - Loss: 6340.4405, Recon: 0.5093, KL: 82.0723\n",
      "  Val   - Loss: 6364.6839, Recon: 0.5114, KL: 80.3254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 391/391 [02:01<00:00,  3.21it/s, Loss=6183.3999, Recon=0.4964, KL=83.7705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/50:\n",
      "  Train - Loss: 6336.9413, Recon: 0.5090, KL: 82.1642\n",
      "  Val   - Loss: 6370.0347, Recon: 0.5117, KL: 81.9173\n",
      "  Saved checkpoint: ./outputs/vae_baseline_latent128_20251231_170110/checkpoints/checkpoint_epoch_25.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 391/391 [02:01<00:00,  3.21it/s, Loss=6372.5830, Recon=0.5119, KL=82.5838]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26/50:\n",
      "  Train - Loss: 6336.9243, Recon: 0.5090, KL: 82.3237\n",
      "  Val   - Loss: 6370.4287, Recon: 0.5117, KL: 82.6841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 391/391 [02:02<00:00,  3.19it/s, Loss=6243.2446, Recon=0.5011, KL=85.5626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27/50:\n",
      "  Train - Loss: 6335.1962, Recon: 0.5089, KL: 82.4300\n",
      "  Val   - Loss: 6361.6811, Recon: 0.5109, KL: 83.5818\n",
      "  ⭐ New best model saved! Val Loss: 6361.6811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 391/391 [02:02<00:00,  3.20it/s, Loss=6181.1880, Recon=0.4964, KL=81.0723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28/50:\n",
      "  Train - Loss: 6332.9836, Recon: 0.5087, KL: 82.5227\n",
      "  Val   - Loss: 6360.1173, Recon: 0.5109, KL: 81.6714\n",
      "  ⭐ New best model saved! Val Loss: 6360.1173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 391/391 [02:03<00:00,  3.17it/s, Loss=6394.1719, Recon=0.5136, KL=83.1226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29/50:\n",
      "  Train - Loss: 6333.3137, Recon: 0.5087, KL: 82.5367\n",
      "  Val   - Loss: 6361.0769, Recon: 0.5110, KL: 81.7419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 391/391 [02:07<00:00,  3.07it/s, Loss=6355.7808, Recon=0.5105, KL=83.2858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30/50:\n",
      "  Train - Loss: 6331.2050, Recon: 0.5085, KL: 82.6543\n",
      "  Val   - Loss: 6363.3299, Recon: 0.5111, KL: 83.0373\n",
      "  Saved checkpoint: ./outputs/vae_baseline_latent128_20251231_170110/checkpoints/checkpoint_epoch_30.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 391/391 [02:01<00:00,  3.21it/s, Loss=6376.1016, Recon=0.5120, KL=84.0521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31/50:\n",
      "  Train - Loss: 6330.0346, Recon: 0.5084, KL: 82.7081\n",
      "  Val   - Loss: 6361.6011, Recon: 0.5109, KL: 83.4094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 391/391 [02:01<00:00,  3.21it/s, Loss=6374.1709, Recon=0.5118, KL=84.8977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32/50:\n",
      "  Train - Loss: 6330.1492, Recon: 0.5084, KL: 82.9407\n",
      "  Val   - Loss: 6358.6386, Recon: 0.5107, KL: 83.2992\n",
      "  ⭐ New best model saved! Val Loss: 6358.6386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 391/391 [02:02<00:00,  3.20it/s, Loss=6442.0059, Recon=0.5175, KL=82.8335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33/50:\n",
      "  Train - Loss: 6328.2123, Recon: 0.5082, KL: 82.9641\n",
      "  Val   - Loss: 6357.0402, Recon: 0.5106, KL: 82.2747\n",
      "  ⭐ New best model saved! Val Loss: 6357.0402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 391/391 [02:02<00:00,  3.19it/s, Loss=6344.3442, Recon=0.5094, KL=84.4618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34/50:\n",
      "  Train - Loss: 6326.5197, Recon: 0.5081, KL: 83.1365\n",
      "  Val   - Loss: 6356.1923, Recon: 0.5105, KL: 83.7558\n",
      "  ⭐ New best model saved! Val Loss: 6356.1923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 391/391 [02:02<00:00,  3.20it/s, Loss=6266.0044, Recon=0.5033, KL=81.6552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35/50:\n",
      "  Train - Loss: 6325.2870, Recon: 0.5080, KL: 83.1755\n",
      "  Val   - Loss: 6361.8120, Recon: 0.5109, KL: 84.4153\n",
      "  Saved checkpoint: ./outputs/vae_baseline_latent128_20251231_170110/checkpoints/checkpoint_epoch_35.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 391/391 [02:02<00:00,  3.19it/s, Loss=6366.0308, Recon=0.5110, KL=86.5999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36/50:\n",
      "  Train - Loss: 6324.7551, Recon: 0.5079, KL: 83.4057\n",
      "  Val   - Loss: 6360.9611, Recon: 0.5109, KL: 83.5519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 391/391 [02:02<00:00,  3.20it/s, Loss=6291.0034, Recon=0.5052, KL=83.0476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37/50:\n",
      "  Train - Loss: 6323.9490, Recon: 0.5079, KL: 83.4119\n",
      "  Val   - Loss: 6355.2878, Recon: 0.5104, KL: 83.6642\n",
      "  ⭐ New best model saved! Val Loss: 6355.2878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 391/391 [02:01<00:00,  3.21it/s, Loss=6387.6929, Recon=0.5131, KL=82.8664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38/50:\n",
      "  Train - Loss: 6323.2962, Recon: 0.5078, KL: 83.5085\n",
      "  Val   - Loss: 6360.7877, Recon: 0.5109, KL: 83.1566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 391/391 [02:01<00:00,  3.21it/s, Loss=6446.3022, Recon=0.5175, KL=87.1290]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39/50:\n",
      "  Train - Loss: 6322.2919, Recon: 0.5077, KL: 83.6065\n",
      "  Val   - Loss: 6359.8396, Recon: 0.5108, KL: 82.7322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 391/391 [02:02<00:00,  3.20it/s, Loss=6272.9976, Recon=0.5038, KL=82.4172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40/50:\n",
      "  Train - Loss: 6321.3256, Recon: 0.5076, KL: 83.6573\n",
      "  Val   - Loss: 6356.1699, Recon: 0.5105, KL: 82.8961\n",
      "  Saved checkpoint: ./outputs/vae_baseline_latent128_20251231_170110/checkpoints/checkpoint_epoch_40.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 391/391 [02:02<00:00,  3.20it/s, Loss=6271.7998, Recon=0.5037, KL=82.7789]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41/50:\n",
      "  Train - Loss: 6319.4581, Recon: 0.5075, KL: 83.7897\n",
      "  Val   - Loss: 6355.3707, Recon: 0.5104, KL: 83.5589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 391/391 [02:02<00:00,  3.20it/s, Loss=6294.2231, Recon=0.5054, KL=83.8687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42/50:\n",
      "  Train - Loss: 6319.3494, Recon: 0.5075, KL: 83.7799\n",
      "  Val   - Loss: 6356.1206, Recon: 0.5105, KL: 82.6730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 391/391 [02:02<00:00,  3.20it/s, Loss=6407.7256, Recon=0.5147, KL=82.7574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43/50:\n",
      "  Train - Loss: 6319.0847, Recon: 0.5074, KL: 83.7517\n",
      "  Val   - Loss: 6356.8568, Recon: 0.5104, KL: 85.3786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 391/391 [02:02<00:00,  3.19it/s, Loss=6184.9902, Recon=0.4966, KL=83.0815]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44/50:\n",
      "  Train - Loss: 6309.3049, Recon: 0.5066, KL: 84.0141\n",
      "  Val   - Loss: 6348.0464, Recon: 0.5098, KL: 83.5108\n",
      "  ⭐ New best model saved! Val Loss: 6348.0464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 391/391 [02:02<00:00,  3.20it/s, Loss=6358.9531, Recon=0.5107, KL=82.9790]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45/50:\n",
      "  Train - Loss: 6308.0440, Recon: 0.5065, KL: 84.0641\n",
      "  Val   - Loss: 6348.3501, Recon: 0.5098, KL: 83.4201\n",
      "  Saved checkpoint: ./outputs/vae_baseline_latent128_20251231_170110/checkpoints/checkpoint_epoch_45.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 391/391 [02:02<00:00,  3.19it/s, Loss=6259.4468, Recon=0.5027, KL=81.8462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46/50:\n",
      "  Train - Loss: 6308.1152, Recon: 0.5065, KL: 84.1240\n",
      "  Val   - Loss: 6348.1996, Recon: 0.5098, KL: 83.5715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 391/391 [02:01<00:00,  3.21it/s, Loss=6534.5088, Recon=0.5247, KL=87.4725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47/50:\n",
      "  Train - Loss: 6307.6595, Recon: 0.5065, KL: 84.2165\n",
      "  Val   - Loss: 6348.8349, Recon: 0.5098, KL: 84.3211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 391/391 [02:02<00:00,  3.20it/s, Loss=6133.4321, Recon=0.4923, KL=83.5972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48/50:\n",
      "  Train - Loss: 6306.8524, Recon: 0.5064, KL: 84.3207\n",
      "  Val   - Loss: 6350.4367, Recon: 0.5100, KL: 84.0097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 391/391 [02:01<00:00,  3.21it/s, Loss=6216.7100, Recon=0.4991, KL=83.5762]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49/50:\n",
      "  Train - Loss: 6306.6552, Recon: 0.5064, KL: 84.3302\n",
      "  Val   - Loss: 6348.7984, Recon: 0.5098, KL: 84.1223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 391/391 [02:02<00:00,  3.20it/s, Loss=6209.8389, Recon=0.4986, KL=82.5892]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50/50:\n",
      "  Train - Loss: 6306.1519, Recon: 0.5063, KL: 84.3299\n",
      "  Val   - Loss: 6348.4335, Recon: 0.5098, KL: 84.1483\n",
      "  Saved checkpoint: ./outputs/vae_baseline_latent128_20251231_170110/checkpoints/checkpoint_epoch_50.pth\n",
      "\n",
      "============================================================\n",
      "Training Complete!\n",
      "============================================================\n",
      "\n",
      "Training curves saved to: ./outputs/vae_baseline_latent128_20251231_170110/training_curves.png\n",
      "Final model saved to: ./outputs/vae_baseline_latent128_20251231_170110/checkpoints/final_model.pth\n",
      "\n",
      "All outputs saved to: ./outputs/vae_baseline_latent128_20251231_170110\n",
      "Best validation loss: 6348.0464\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "VAE Implementation for CelebA Dataset - Portfolio 3 Phase 1\n",
    "Baseline model with latent_dim = 128\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Dataset Class\n",
    "# ============================================================================\n",
    "\n",
    "class CelebADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for CelebA aligned and cropped images\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, partition_file, split='train', transform=None, subset_size=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir: Path to CelebA directory\n",
    "            partition_file: Path to list_eval_partition.txt\n",
    "            split: 'train', 'val', or 'test' (0, 1, 2 in partition file)\n",
    "            transform: Transformations to apply\n",
    "            subset_size: If specified, use only this many images (for faster training)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.img_dir = os.path.join(root_dir, 'Img', 'img_align_celeba')\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load partition information\n",
    "        self.image_files = []\n",
    "        split_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        split_id = split_map[split]\n",
    "        \n",
    "        with open(partition_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 2:\n",
    "                    img_name, partition = parts\n",
    "                    if int(partition) == split_id:\n",
    "                        self.image_files.append(img_name)\n",
    "        \n",
    "        # Apply subset if specified\n",
    "        if subset_size is not None and subset_size < len(self.image_files):\n",
    "            np.random.seed(42)\n",
    "            indices = np.random.choice(len(self.image_files), subset_size, replace=False)\n",
    "            self.image_files = [self.image_files[i] for i in sorted(indices)]\n",
    "        \n",
    "        print(f\"{split.upper()} set: {len(self.image_files)} images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VAE Model Architecture\n",
    "# ============================================================================\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Variational Autoencoder for CelebA (64x64 images)\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=128, img_channels=3):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # ====== ENCODER ======\n",
    "        # Input: (3, 64, 64)\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Layer 1: (3, 64, 64) -> (32, 32, 32)\n",
    "            nn.Conv2d(img_channels, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Layer 2: (32, 32, 32) -> (64, 16, 16)\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Layer 3: (64, 16, 16) -> (128, 8, 8)\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Layer 4: (128, 8, 8) -> (256, 4, 4)\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Flattened feature size: 256 * 4 * 4 = 4096\n",
    "        self.fc_encoder = nn.Sequential(\n",
    "            nn.Linear(256 * 4 * 4, 512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Latent space parameters\n",
    "        self.fc_mu = nn.Linear(512, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512, latent_dim)\n",
    "        \n",
    "        # ====== DECODER ======\n",
    "        self.fc_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 256 * 4 * 4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Decoder: Mirror of encoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Layer 1: (256, 4, 4) -> (128, 8, 8)\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Layer 2: (128, 8, 8) -> (64, 16, 16)\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Layer 3: (64, 16, 16) -> (32, 32, 32)\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Layer 4: (32, 32, 32) -> (3, 64, 64)\n",
    "            nn.ConvTranspose2d(32, img_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()  # Output in [0, 1]\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input to latent parameters\"\"\"\n",
    "        h = self.encoder(x)\n",
    "        h = h.view(h.size(0), -1)  # Flatten\n",
    "        h = self.fc_encoder(h)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick: z = mu + std * epsilon\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent vector to image\"\"\"\n",
    "        h = self.fc_decoder(z)\n",
    "        h = h.view(h.size(0), 256, 4, 4)  # Reshape\n",
    "        return self.decoder(h)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Full forward pass\"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, logvar\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Loss Function\n",
    "# ============================================================================\n",
    "def vae_loss(recon_x, x, mu, logvar, beta=1.0):\n",
    "    \"\"\"\n",
    "    VAE Loss - Production Ready Version\n",
    "    \"\"\"\n",
    "    batch_size = x.size(0)\n",
    "    \n",
    "    # Reconstruction: Binary cross-entropy\n",
    "    # Sum over pixels, mean over batch\n",
    "    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum') / batch_size\n",
    "    # Per-pixel for display\n",
    "    recon_loss_display = recon_loss / (x.size(1) * x.size(2) * x.size(3))\n",
    "    \n",
    "    # KL Divergence\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / batch_size\n",
    "    \n",
    "    total_loss = recon_loss + beta * kl_loss\n",
    "    \n",
    "    return total_loss, recon_loss_display, kl_loss\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Training Function\n",
    "# ============================================================================\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, epoch, beta=1.0):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_recon_loss = 0\n",
    "    train_kl_loss = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "    for batch_idx, data in enumerate(pbar):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, recon_loss, kl_loss = vae_loss(recon_batch, data, mu, logvar, beta)\n",
    "        \n",
    "        # SAFETY CHECK: Detect posterior collapse\n",
    "        if kl_loss.item() < 1.0 and epoch > 1:\n",
    "            print(f\"\\n⚠️ WARNING: KL collapsed at epoch {epoch}, batch {batch_idx}\")\n",
    "            print(f\"   KL: {kl_loss.item():.6f}\")\n",
    "            print(f\"   mu range: [{mu.min():.4f}, {mu.max():.4f}]\")\n",
    "            print(f\"   logvar range: [{logvar.min():.4f}, {logvar.max():.4f}]\")\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate losses\n",
    "        train_loss += loss.item()\n",
    "        train_recon_loss += recon_loss.item()\n",
    "        train_kl_loss += kl_loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{loss.item():.4f}',\n",
    "            'Recon': f'{recon_loss.item():.4f}',\n",
    "            'KL': f'{kl_loss.item():.4f}'\n",
    "        })\n",
    "    \n",
    "    # Average losses\n",
    "    n_batches = len(train_loader)\n",
    "    return train_loss / n_batches, train_recon_loss / n_batches, train_kl_loss / n_batches\n",
    "\n",
    "\n",
    "def validate(model, val_loader, beta=1.0):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_recon_loss = 0\n",
    "    val_kl_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss, recon_loss, kl_loss = vae_loss(recon_batch, data, mu, logvar, beta)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_recon_loss += recon_loss.item()\n",
    "            val_kl_loss += kl_loss.item()\n",
    "    \n",
    "    # Average losses\n",
    "    n_batches = len(val_loader)\n",
    "    return val_loss / n_batches, val_recon_loss / n_batches, val_kl_loss / n_batches\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Visualization Functions\n",
    "# ============================================================================\n",
    "\n",
    "def save_sample_images(model, val_loader, epoch, save_dir, num_samples=8):\n",
    "    \"\"\"Save sample reconstructions\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch\n",
    "    data = next(iter(val_loader))[:num_samples].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        recon, _, _ = model(data)\n",
    "    \n",
    "    # Move to CPU and denormalize\n",
    "    data = data.cpu()\n",
    "    recon = recon.cpu()\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(num_samples * 2, 4))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Original\n",
    "        axes[0, i].imshow(data[i].permute(1, 2, 0))\n",
    "        axes[0, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[0, i].set_title('Original', fontsize=10)\n",
    "        \n",
    "        # Reconstruction\n",
    "        axes[1, i].imshow(recon[i].permute(1, 2, 0))\n",
    "        axes[1, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[1, i].set_title('Reconstructed', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'reconstruction_epoch_{epoch}.png'), dpi=100, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_training_curves(history, save_dir):\n",
    "    \"\"\"Plot and save training curves\"\"\"\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Total Loss\n",
    "    axes[0].plot(epochs, history['train_loss'], label='Train', linewidth=2)\n",
    "    axes[0].plot(epochs, history['val_loss'], label='Validation', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Total Loss')\n",
    "    axes[0].set_title('Total Loss (ELBO)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Reconstruction Loss\n",
    "    axes[1].plot(epochs, history['train_recon'], label='Train', linewidth=2)\n",
    "    axes[1].plot(epochs, history['val_recon'], label='Validation', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Reconstruction Loss')\n",
    "    axes[1].set_title('Reconstruction Loss (BCE)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # KL Divergence\n",
    "    axes[2].plot(epochs, history['train_kl'], label='Train', linewidth=2)\n",
    "    axes[2].plot(epochs, history['val_kl'], label='Validation', linewidth=2)\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('KL Divergence')\n",
    "    axes[2].set_title('KL Divergence')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'training_curves.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Main Training Script\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    # ========== Configuration ==========\n",
    "    config = {\n",
    "        'latent_dim': 128,\n",
    "        'beta': 1.0,  \n",
    "        'img_size': 64,\n",
    "        'batch_size': 128,\n",
    "        'num_epochs': 100,\n",
    "        'learning_rate': 1e-3,\n",
    "        'subset_train': 50000,  # Use 50k training images \n",
    "        'subset_val': 5000,     # Use 5k validation images\n",
    "        'num_workers': 0,       # DataLoader workers\n",
    "        'save_interval': 5,     # Save checkpoint every N epochs\n",
    "    }\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"VAE Training Configuration\")\n",
    "    print(\"=\" * 60)\n",
    "    for key, value in config.items():\n",
    "        print(f\"{key:20s}: {value}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ========== Paths ==========\n",
    "    celeba_root = './CelebA'  # Adjust if needed\n",
    "    partition_file = os.path.join(celeba_root, 'Eval', 'list_eval_partition.txt')\n",
    "    \n",
    "    # Create output directories\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = f'./outputs/vae_baseline_latent{config[\"latent_dim\"]}_{timestamp}'\n",
    "    checkpoint_dir = os.path.join(output_dir, 'checkpoints')\n",
    "    sample_dir = os.path.join(output_dir, 'samples')\n",
    "    \n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(sample_dir, exist_ok=True)\n",
    "    \n",
    "    # Save configuration\n",
    "    with open(os.path.join(output_dir, 'config.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    # ========== Data Transforms ==========\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(config['img_size']),\n",
    "        transforms.CenterCrop(config['img_size']),\n",
    "        transforms.ToTensor(),\n",
    "        # Images are already in [0, 1] after ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # ========== Datasets and DataLoaders ==========\n",
    "    print(\"\\nLoading datasets...\")\n",
    "    train_dataset = CelebADataset(\n",
    "        root_dir=celeba_root,\n",
    "        partition_file=partition_file,\n",
    "        split='train',\n",
    "        transform=transform,\n",
    "        subset_size=config['subset_train']\n",
    "    )\n",
    "    \n",
    "    val_dataset = CelebADataset(\n",
    "        root_dir=celeba_root,\n",
    "        partition_file=partition_file,\n",
    "        split='val',\n",
    "        transform=transform,\n",
    "        subset_size=config['subset_val']\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # ========== Model Setup ==========\n",
    "    print(\"\\nInitializing model...\")\n",
    "    model = VAE(latent_dim=config['latent_dim']).to(device)\n",
    "    \n",
    "    # Print model summary\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5\n",
    "    )\n",
    "    \n",
    "    # ========== Training Loop ==========\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Starting Training\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_recon': [], 'train_kl': [],\n",
    "        'val_loss': [], 'val_recon': [], 'val_kl': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(1, config['num_epochs'] + 1):\n",
    "        # Train\n",
    "        train_loss, train_recon, train_kl = train_epoch(\n",
    "            model, train_loader, optimizer, epoch, beta=config['beta']\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_recon, val_kl = validate(model, val_loader, beta=config['beta'])\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_recon'].append(train_recon)\n",
    "        history['train_kl'].append(train_kl)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_recon'].append(val_recon)\n",
    "        history['val_kl'].append(val_kl)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch}/{config['num_epochs']}:\")\n",
    "        print(f\"  Train - Loss: {train_loss:.4f}, Recon: {train_recon:.4f}, KL: {train_kl:.4f}\")\n",
    "        print(f\"  Val   - Loss: {val_loss:.4f}, Recon: {val_recon:.4f}, KL: {val_kl:.4f}\")\n",
    "        \n",
    "        # HEALTH CHECK\n",
    "        if val_kl < 10.0:\n",
    "            print(f\"  ⚠️  WARNING: KL very low ({val_kl:.4f}) - monitor for collapse\")\n",
    "        elif val_kl > 200.0:\n",
    "            print(f\"  ⚠️  WARNING: KL very high ({val_kl:.4f}) - model may ignore inputs\")\n",
    "        \n",
    "        # Save sample reconstructions\n",
    "        if epoch % config['save_interval'] == 0:\n",
    "            save_sample_images(model, val_loader, epoch, sample_dir)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if epoch % config['save_interval'] == 0 or epoch == config['num_epochs']:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'config': config,\n",
    "                'history': history,\n",
    "            }\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print(f\"  Saved checkpoint: {checkpoint_path}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'config': config,\n",
    "                'history': history,\n",
    "                'val_loss': val_loss,\n",
    "            }\n",
    "            best_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
    "            torch.save(best_checkpoint, best_path)\n",
    "            print(f\"  ⭐ New best model saved! Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # ========== Final Steps ==========\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Plot training curves\n",
    "    plot_training_curves(history, output_dir)\n",
    "    print(f\"\\nTraining curves saved to: {output_dir}/training_curves.png\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_checkpoint = {\n",
    "        'epoch': config['num_epochs'],\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'config': config,\n",
    "        'history': history,\n",
    "    }\n",
    "    final_path = os.path.join(checkpoint_dir, 'final_model.pth')\n",
    "    torch.save(final_checkpoint, final_path)\n",
    "    print(f\"Final model saved to: {final_path}\")\n",
    "    \n",
    "    # Save training history\n",
    "    with open(os.path.join(output_dir, 'training_history.json'), 'w') as f:\n",
    "        json.dump(history, f, indent=4)\n",
    "    \n",
    "    print(f\"\\nAll outputs saved to: {output_dir}\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a44a47cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VAE Model Testing and Visualization\n",
      "============================================================\n",
      "\n",
      "Using output directory: ./outputs/vae_baseline_latent128_20251231_170110\n",
      "Loading BEST model: ./outputs/vae_baseline_latent128_20251231_170110/checkpoints/best_model.pth\n",
      "\n",
      "============================================================\n",
      "Loading Trained Model\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'VAE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     61\u001b[39m config = checkpoint[\u001b[33m'\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Create model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m model = \u001b[43mVAE\u001b[49m(latent_dim=config[\u001b[33m'\u001b[39m\u001b[33mlatent_dim\u001b[39m\u001b[33m'\u001b[39m]).to(device)\n\u001b[32m     65\u001b[39m model.load_state_dict(checkpoint[\u001b[33m'\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     66\u001b[39m model.eval()\n",
      "\u001b[31mNameError\u001b[39m: name 'VAE' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECOND CELL - VAE Testing and Visualization\n",
    "\"\"\"\n",
    "device = 'cpu'\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Reuse the VAE and Dataset classes from first cell (already defined)\n",
    "# No need to redefine them\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VAE Model Testing and Visualization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Find the most recent output directory\n",
    "output_dirs = glob.glob('./outputs/vae_baseline_latent128_*')\n",
    "if not output_dirs:\n",
    "    raise FileNotFoundError(\"No output directories found! Make sure training completed.\")\n",
    "\n",
    "output_dir = sorted(output_dirs)[-1]  # Get most recent\n",
    "checkpoint_dir = os.path.join(output_dir, 'checkpoints')\n",
    "print(f\"\\nUsing output directory: {output_dir}\")\n",
    "\n",
    "# Check which checkpoint to use\n",
    "best_model_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
    "final_model_path = os.path.join(checkpoint_dir, 'final_model.pth')\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    checkpoint_path = best_model_path\n",
    "    print(f\"Loading BEST model: {checkpoint_path}\")\n",
    "elif os.path.exists(final_model_path):\n",
    "    checkpoint_path = final_model_path\n",
    "    print(f\"Loading FINAL model: {checkpoint_path}\")\n",
    "else:\n",
    "    # Find latest checkpoint\n",
    "    checkpoints = glob.glob(os.path.join(checkpoint_dir, 'checkpoint_epoch_*.pth'))\n",
    "    if checkpoints:\n",
    "        checkpoint_path = sorted(checkpoints)[-1]\n",
    "        print(f\"Loading latest checkpoint: {checkpoint_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No checkpoints found!\")\n",
    "\n",
    "# ============================================================================\n",
    "# Load Model\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Loading Trained Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "config = checkpoint['config']\n",
    "\n",
    "# Create model\n",
    "model = VAE(latent_dim=config['latent_dim']).to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"✅ Model loaded successfully!\")\n",
    "print(f\"   Epoch: {checkpoint['epoch']}\")\n",
    "print(f\"   Latent dimension: {config['latent_dim']}\")\n",
    "if 'val_loss' in checkpoint:\n",
    "    print(f\"   Validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Load Test Dataset\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Loading Test Dataset\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "celeba_root = './CelebA'\n",
    "partition_file = os.path.join(celeba_root, 'Eval', 'list_eval_partition.txt')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(config['img_size']),\n",
    "    transforms.CenterCrop(config['img_size']),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_dataset = CelebADataset(\n",
    "    root_dir=celeba_root,\n",
    "    partition_file=partition_file,\n",
    "    split='test',\n",
    "    transform=transform,\n",
    "    subset_size=2000  # Use 2000 test images\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"✅ Test dataset loaded: {len(test_dataset)} images\")\n",
    "\n",
    "# ============================================================================\n",
    "# Visualization 1: Reconstructions\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Generating Reconstructions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def visualize_reconstructions(model, dataset, num_samples=10):\n",
    "    \"\"\"Visualize original and reconstructed images\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get random samples\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(num_samples * 2, 4))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(indices):\n",
    "            img = dataset[idx].unsqueeze(0).to(device)\n",
    "            recon, _, _ = model(img)\n",
    "            \n",
    "            # Original\n",
    "            img_np = img[0].cpu().permute(1, 2, 0).numpy()\n",
    "            axes[0, i].imshow(img_np)\n",
    "            axes[0, i].axis('off')\n",
    "            if i == 0:\n",
    "                axes[0, i].set_ylabel('Original', fontsize=12, fontweight='bold')\n",
    "            \n",
    "            # Reconstruction\n",
    "            recon_np = recon[0].cpu().permute(1, 2, 0).numpy()\n",
    "            axes[1, i].imshow(recon_np)\n",
    "            axes[1, i].axis('off')\n",
    "            if i == 0:\n",
    "                axes[1, i].set_ylabel('Reconstructed', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('VAE Reconstructions on Test Set', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig1 = visualize_reconstructions(model, test_dataset, num_samples=10)\n",
    "print(\"✅ Reconstructions displayed above\")\n",
    "\n",
    "# ============================================================================\n",
    "# Visualization 2: Random Samples from Latent Space\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Generating Random Samples from Latent Space\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def generate_random_samples(model, num_samples=16):\n",
    "    \"\"\"Generate random samples from the latent space\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Sample from standard normal\n",
    "        z = torch.randn(num_samples, model.latent_dim).to(device)\n",
    "        samples = model.decode(z).cpu()\n",
    "    \n",
    "    # Plot\n",
    "    grid_size = int(np.sqrt(num_samples))\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(grid_size * 2.5, grid_size * 2.5))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        sample_np = samples[i].permute(1, 2, 0).numpy()\n",
    "        axes[i].imshow(sample_np)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Random Samples from Latent Space (z ~ N(0,I))', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig2 = generate_random_samples(model, num_samples=16)\n",
    "print(\"✅ Random samples displayed above\")\n",
    "\n",
    "# ============================================================================\n",
    "# Visualization 3: Latent Space Interpolation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Generating Latent Space Interpolations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def interpolate_latent(model, dataset, num_pairs=3, num_steps=10):\n",
    "    \"\"\"Interpolate between pairs of images in latent space\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(num_pairs, num_steps, figsize=(num_steps * 2, num_pairs * 2))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for pair_idx in range(num_pairs):\n",
    "            # Get two random images\n",
    "            idx1, idx2 = np.random.choice(len(dataset), 2, replace=False)\n",
    "            img1 = dataset[idx1].unsqueeze(0).to(device)\n",
    "            img2 = dataset[idx2].unsqueeze(0).to(device)\n",
    "            \n",
    "            # Encode to latent space\n",
    "            mu1, _ = model.encode(img1)\n",
    "            mu2, _ = model.encode(img2)\n",
    "            \n",
    "            # Interpolate\n",
    "            alphas = np.linspace(0, 1, num_steps)\n",
    "            \n",
    "            for step_idx, alpha in enumerate(alphas):\n",
    "                # Linear interpolation in latent space\n",
    "                z_interp = (1 - alpha) * mu1 + alpha * mu2\n",
    "                \n",
    "                # Decode\n",
    "                img_interp = model.decode(z_interp)\n",
    "                \n",
    "                # Display\n",
    "                img_np = img_interp[0].cpu().permute(1, 2, 0).numpy()\n",
    "                \n",
    "                if num_pairs == 1:\n",
    "                    axes[step_idx].imshow(img_np)\n",
    "                    axes[step_idx].axis('off')\n",
    "                    if step_idx == 0:\n",
    "                        axes[step_idx].set_title('Start', fontsize=10)\n",
    "                    elif step_idx == num_steps - 1:\n",
    "                        axes[step_idx].set_title('End', fontsize=10)\n",
    "                else:\n",
    "                    axes[pair_idx, step_idx].imshow(img_np)\n",
    "                    axes[pair_idx, step_idx].axis('off')\n",
    "                    \n",
    "                    if step_idx == 0:\n",
    "                        axes[pair_idx, step_idx].set_ylabel(f'Pair {pair_idx+1}', fontsize=10, fontweight='bold')\n",
    "                    if pair_idx == 0:\n",
    "                        if step_idx == 0:\n",
    "                            axes[pair_idx, step_idx].set_title('Start', fontsize=10)\n",
    "                        elif step_idx == num_steps - 1:\n",
    "                            axes[pair_idx, step_idx].set_title('End', fontsize=10)\n",
    "    \n",
    "    plt.suptitle('Latent Space Interpolations (Smooth Transitions)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig3 = interpolate_latent(model, test_dataset, num_pairs=3, num_steps=10)\n",
    "print(\"✅ Interpolations displayed above\")\n",
    "\n",
    "# ============================================================================\n",
    "# Visualization 4: Latent Dimension Traversals\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Generating Latent Dimension Traversals\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def traverse_latent_dims(model, dataset, num_dims=10, num_steps=7, traversal_range=3.0):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get a random image and encode it\n",
    "        idx = np.random.choice(len(dataset))\n",
    "        img = dataset[idx].unsqueeze(0).to(device)\n",
    "        mu, _ = model.encode(img)\n",
    "        \n",
    "        # Select random dimensions to traverse\n",
    "        dims_to_traverse = np.random.choice(model.latent_dim, num_dims, replace=False)\n",
    "        \n",
    "        fig, axes = plt.subplots(num_dims, num_steps, figsize=(num_steps * 1.8, num_dims * 1.8))\n",
    "        \n",
    "        traversal_values = np.linspace(-traversal_range, traversal_range, num_steps)\n",
    "        \n",
    "        for dim_idx, dim in enumerate(dims_to_traverse):\n",
    "            for step_idx, value in enumerate(traversal_values):\n",
    "                # Copy the latent vector and modify one dimension\n",
    "                z_modified = mu.clone()\n",
    "                z_modified[0, dim] = value\n",
    "                \n",
    "                # Decode\n",
    "                img_generated = model.decode(z_modified)\n",
    "                img_np = img_generated[0].cpu().permute(1, 2, 0).numpy()\n",
    "                \n",
    "                # Display\n",
    "                axes[dim_idx, step_idx].imshow(img_np)\n",
    "                axes[dim_idx, step_idx].axis('off')\n",
    "                \n",
    "                if step_idx == 0:\n",
    "                    axes[dim_idx, step_idx].set_ylabel(f'Dim {dim}', fontsize=9, fontweight='bold')\n",
    "                \n",
    "                if dim_idx == 0:\n",
    "                    axes[dim_idx, step_idx].set_title(f'{value:.1f}', fontsize=9)\n",
    "        \n",
    "        plt.suptitle(f'Latent Dimension Traversals (Range: [{-traversal_range:.1f}, {traversal_range:.1f}])', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig4 = traverse_latent_dims(model, test_dataset, num_dims=10, num_steps=7, traversal_range=7.0) #TOBECHANGEDANDCHECKED - abcd\n",
    "print(\"✅ Latent traversals displayed above\")\n",
    "print(\"   Each row shows how varying a single latent dimension affects the generated face\")\n",
    "\n",
    "# ============================================================================\n",
    "# Compute Reconstruction Quality Metrics\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Computing Reconstruction Quality Metrics\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def compute_reconstruction_metrics(model, test_loader, num_batches=10):\n",
    "    \"\"\"Compute average reconstruction metrics on test set\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    mse_total = 0\n",
    "    mae_total = 0\n",
    "    n_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            if batch_idx >= num_batches:\n",
    "                break\n",
    "            \n",
    "            data = data.to(device)\n",
    "            recon, _, _ = model(data)\n",
    "            \n",
    "            # MSE (Mean Squared Error)\n",
    "            mse = torch.mean((recon - data) ** 2, dim=[1, 2, 3])\n",
    "            mse_total += mse.sum().item()\n",
    "            \n",
    "            # MAE (Mean Absolute Error)\n",
    "            mae = torch.mean(torch.abs(recon - data), dim=[1, 2, 3])\n",
    "            mae_total += mae.sum().item()\n",
    "            \n",
    "            n_samples += data.size(0)\n",
    "    \n",
    "    avg_mse = mse_total / n_samples\n",
    "    avg_mae = mae_total / n_samples\n",
    "    \n",
    "    return avg_mse, avg_mae\n",
    "\n",
    "avg_mse, avg_mae = compute_reconstruction_metrics(model, test_loader, num_batches=10)\n",
    "\n",
    "print(f\"✅ Reconstruction Quality Metrics (on {10} test batches):\")\n",
    "print(f\"   Mean Squared Error (MSE): {avg_mse:.6f}\")\n",
    "print(f\"   Mean Absolute Error (MAE): {avg_mae:.6f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Summary\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Testing Complete - Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n📊 Model Information:\")\n",
    "print(f\"   Latent Dimension: {config['latent_dim']}\")\n",
    "print(f\"   Image Size: {config['img_size']}×{config['img_size']}\")\n",
    "print(f\"   Training Epochs: {checkpoint['epoch']}\")\n",
    "print(f\"   Beta (β): {config['beta']}\")\n",
    "\n",
    "print(f\"\\n📈 Performance:\")\n",
    "if 'val_loss' in checkpoint:\n",
    "    print(f\"   Best Validation Loss: {checkpoint['val_loss']:.4f}\")\n",
    "print(f\"   Test MSE: {avg_mse:.6f}\")\n",
    "print(f\"   Test MAE: {avg_mae:.6f}\")\n",
    "\n",
    "print(f\"\\n✅ Generated Visualizations:\")\n",
    "print(f\"   1. ✓ Original vs. Reconstructed (10 samples)\")\n",
    "print(f\"   2. ✓ Random Samples from Latent Space (16 samples)\")\n",
    "print(f\"   3. ✓ Latent Space Interpolations (3 pairs)\")\n",
    "print(f\"   4. ✓ Latent Dimension Traversals (10 dimensions)\")\n",
    "\n",
    "print(f\"\\n💾 Model checkpoint location:\")\n",
    "print(f\"   {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
